[DATASET]
filters = []
image_size = 224
net_input_size = 224
dataset_means = [0.63695754, 0.54695442, 0.50487832]
dataset_stds = [0.27612603, 0.27153761, 0.2515826]
#train mean: [162.42417172 139.47337593 128.74397056], NORMALIZED: [0.63695754 0.54695442 0.50487832]
#train std: [70.4121388  69.24209172 64.15356255], NORMALIZED: [0.27612603 0.27153761 0.2515826 ]
# following parameter is the name of the important transforms
# the required parameters for the transforms should be accessed in the transforms_config.py file
transforms_type=mtcnn
# TODO - after running check if need to fix.
# parmeters for transforms # TODO - after running check if need to fix.
transforms_prob=0.7
rotation_angle=16
blur_kernel_size=7
blur_min_std=1
blur_max_std=15
max_noise_variance=15
min_noise_variance=1
# set this dir to the dataset dir
raw_dataset_path = /home/ssd_storage/datasets/students/expertise/Experiment_1/hands_only
crop_scale={"max": 1.0, "min": 1.0}
processed_dataset_root = /home/ssd_storage/datasets/students/expertise/Experiment_1/hands_only
class_filter_dataset_dir = phase_perc_size
# following parameter is a name for the created directory
dataset_name = hands_only
phase_size_dict = {"train":0.8, "val": 0.2}

[MODELLING]
feature_parallelized_architectures = ["VGG", "vgg11", "vgg11_bn", "vgg13", "vgg13_bn", "vgg16", "vgg16_bn",
    "vgg19_bn", "vgg19", "AlexNet", "alexnet"]
architecture = resnet50
# If you want to start from the middle of training, set this to the epoch you wish to start from (it will load start_epoch-1 from the dir)
start_epoch = 0
end_epoch = 120
num_batches_per_epoch_limit=1000
batch_size=128
is_pretrained = False
# NOAM: following parameter should be the same number of trained classes
num_classes = 189
criterion_name = CrossEntropyLoss
criterion_params = {}
workers=4
performance_test=standard
#performance_test=None
perf_threshold=0.99

#each #num_epochs_to_test we make a LFW test
num_epochs_to_test=5


# [OPTIMIZING]
# # NOAM: Same class name as in torch.optim
# optimizer = SGD
# # NOAM: Same parameters as in the object constructor:
# optimizer_params = {
#     "lr": 0.005,
#     "momentum": 0.9,
#     "weight_decay": 1e-3}
# # NOAM: Same class name as in torch.optim.lr_scheduler
# lr_scheduler = StepLR
# # NOAM: Same parameters as in the object constructor:
# lr_scheduler_params = {
#     "step_size": 50,
#     "gamma": 0.1}

# [OPTIMIZING]
# optimizer = Adam
# optimizer_params = {
#     "lr": 1e-3}
# lr_scheduler = StepLR
# lr_scheduler_params = {
#     "step_size": 20,
#     "gamma": 0.2}

[OPTIMIZING]
optimizer = Adam
optimizer_params = {
    "lr": 5e-4,
    "weight_decay": 5e-4
    }
lr_scheduler = StepLR
lr_scheduler_params = {
    "step_size": 20,
    "gamma": 0.4}



[GENERAL]
root_dir = /home/ssd_storage/experiments/students/expertise
#change to name of experiment (the output folder will be created accordingly)
# NOAM: change experiment name to a meaningful name
experiment_name = exp1_hands_only_try2



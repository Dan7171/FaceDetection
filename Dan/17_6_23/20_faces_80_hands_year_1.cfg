[DATASET]
filters = []
image_size = 224
net_input_size = 224
dataset_means = [0.61804861, 0.52281071, 0.47783201] 
dataset_stds = [0.27095917, 0.26033208, 0.2417055 ] 

transforms_type=mtcnn 
#TODO - after running check if need to fix.
# parmeters for transforms # TODO - after running check if need to fix.
transforms_prob=0.7
rotation_angle=16
blur_kernel_size=7
blur_min_std=1
blur_max_std=15
max_noise_variance=15
min_noise_variance=1
# set this dir to the dataset dir
# raw_dataset_path = /home/ssd_storage/datasets/students/expertise/Experiment_1/Hands_and_faces_189_each
raw_dataset_path = /home/ssd_storage/datasets/students/expertise/Experiment_2_fixed/year_1_80_hands

crop_scale={"max": 1.0, "min": 1.0}
# processed_dataset_root = /home/ssd_storage/datasets/students/expertise/Experiment_1/Hands_and_faces_189_each
processed_dataset_root = /home/ssd_storage/datasets/students/expertise/Experiment_2_fixed/year_1_80_hands


class_filter_dataset_dir = phase_perc_size
# following parameter is a name for the created directory
# dataset_name = Hands_and_faces_189_each
dataset_name = year_1
phase_size_dict = {"train":0.8, "val": 0.2}

[MODELLING]
feature_parallelized_architectures = ["VGG", "vgg11", "vgg11_bn", "vgg13", "vgg13_bn", "vgg16", "vgg16_bn",
    "vgg19_bn", "vgg19", "AlexNet", "alexnet"]
architecture = resnet50
# If you want to start from the middle of training, set this to the epoch you wish to start from (it will load start_epoch-1 from the dir)
start_epoch = 0
#TODO change in exp with ratios
# end_epoch = 120
end_epoch = 50
num_batches_per_epoch_limit=1000
batch_size=128
is_pretrained = False
# NOAM: following parameter should be the same number of trained classes
num_classes = 100
#TODO change in 2 sub exp of exp without ratios
criterion_name = CrossEntropyLoss 
criterion_params = {}
workers=4
#performance_test=standard
performance_test=None 
#TODO need to change to standard - understand why bug (maybe because need test images)
perf_threshold=0.99

#each #num_epochs_to_test we make a LFW test
num_epochs_to_test=5 
#TODO ok?

# NOAM: if starting from a pretrained model (like when finetuning) - the weights will be loaded from the given path:
# NOAM: when starting from pretrained model make sure the num_classes parameter is the same number of classes as in the
# weights in the file

# checkpoint_path= # no checkpoint - this is year 1 


[OPTIMIZING] 
#TODO - after running - check if need to fix
# NOAM: Same class name as in torch.optim
optimizer = SGD
# NOAM: Same parameters as in the object constructor:
optimizer_params = {
    "lr":5e-4,
    "weight_decay": 5e-4}
# NOAM: Same class name as in torch.optim.lr_scheduler
lr_scheduler = StepLR
# NOAM: Same parameters as in the object constructor:
lr_scheduler_params = {
    "step_size": 50,
    "gamma": 0.1}
    #     "step_size": 10,

# # FINETUNING section is only relevant when finetuning:
# [FINETUNING] 
# #TODO 
# classes_mode=replace
# #replace/append/skip
# num_classes=378
# #NOAM: The freeze_end parameter is the index of the layer we want to start training from,
# #i.e. if vgg16 is made of 42 consecutive torch.nn.Module objects (you can iterate a torch.nn.Module object's module() function)
# #then layer 40 is fc7
# #example: following code should print "(40, torch.nn.Linear(in_dim=4096, out_dim=4096))"
# #vgg16 = torchvision.models.vgg16()
# #for i, layer in iterate(vgg16.modules())
# #    print(i, type(layer))
# #
# freeze_end=0
# #42-fc8, 40-fc7, 35-fc6, 26-conv5, 19-conv4, 12-conv3, 7-conv2, 0-conv1

[GENERAL]
root_dir = /home/ssd_storage/experiments/students/expertise 
#change to name of experiment (the output folder will be created accordingly)
# NOAM: change experiment name to a meaningful name
# experiment_name = exp1_189each
experiment_name = Experiment_2_year_1 
 